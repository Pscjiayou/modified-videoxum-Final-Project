# Vision LLM-based Cross-modal Summarization Framework for Long-range Videos
## Introduction
In this project, based on the state-of-the-art vision language model [BLIP](https://github.com/salesforce/BLIP), we introduce a local attention modile and developed a framework especially for video summarisation tasks, which provides

- Better understanding of **long-range video**
- **Less repeated segment** in the summarization
- Semantically more **interpretable** and **human-readable** text

## Reference

